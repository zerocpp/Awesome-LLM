(Rafailov et al., Stanford University, NeurIPS 2023 oral)
https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html
[paper](../attachments/NeurIPS-2023-direct-preference-optimization-your-language-model-is-secretly-a-reward-model-Paper-Conference.pdf)

# Abstract
```
本文提出了一种新的名为“直接偏好优化”（DPO）的算法，用于直接从人类的偏好中训练语言模型，无需显式奖励建模或强化学习。与现有的基于强化学习的人类反馈强化学习（RLHF）方法不同，DPO 利用了奖励函数和最优策略之间的映射关系，将人类偏好数据直接转化为语言模型的优化目标。实验证明，DPO 在情感控制、摘要和对话等任务上，与现有的 RLHF 方法相比，表现至少一样好，甚至更好。DPO 算法更加稳定、高效，而且计算量更小。
```

# Figures & Tables
# Citation
```
@inproceedings{NEURIPS2023_a85b405e,
 author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {53728--53741},
 publisher = {Curran Associates, Inc.},
 title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}
```
