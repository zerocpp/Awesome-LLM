# 大模型幻觉（LLM Hallucination）
- [TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space](notes/TruthX.md) (Zhang et al., 中科院计算所, ACL 2024)
- [Can Knowledge Editing Really Correct Hallucinations?](notes/Can%20Knowledge%20Editing%20Really%20Correct%20Hallucinations?.md)(Huang et al., IIT)

# 大模型水印（LLM Watermarking）
- [Scalable watermarking for identifying large language model outputs](notes/synthid-text.md) (Dathathri et al., DeepMind, Nature 2024)

# 大模型对齐（LLM Alignment）
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](notes/DPO.md)(Rafailov et al., Stanford University, NeurIPS 2023 oral)

# 模型编辑（Model Editing）
- [(Talk)模型编辑导致模型崩溃](notes/(Talk)模型编辑导致模型崩溃.md)(杨万里 et al., 中科院计算所, ACL 2024 Findings & EMNLP 2024 Findings)

