# 大模型幻觉（LLM Hallucination）
- [TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space](notes/TruthX.md) (Zhang et al., 中科院计算所, ACL 2024)

# 大模型水印（LLM Watermarking）
- [Scalable watermarking for identifying large language model outputs](notes/synthid-text.md) (Dathathri et al., DeepMind, Nature 2024)

# 大模型对齐（LLM Alignment）
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](notes/DPO.md)(Rafailov et al., Stanford University, NeurIPS 2023 oral)

# 模型编辑（Model Editing）
- [模型编辑的蝴蝶效应](notes/模型编辑的蝴蝶效应.md)(杨万里 et al., 中科院计算所, ACL 2024 Findings)


